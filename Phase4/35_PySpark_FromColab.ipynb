{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_cd_YKidmcH"
      },
      "source": [
        "# Installing PySpark on Google Colab\n",
        "\n",
        "Special thanks to my colleagues Jeff & James for content in this notebook <3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIyBO-9sCxD7"
      },
      "source": [
        "!apt update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22vJMUf5CxzI"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crqUuL1tC31T"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-1.8.0-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsOcaPcRC6gK"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scohCVbjhIH4"
      },
      "source": [
        "# Resilient Distributed Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzG2AoAQgx-B"
      },
      "source": [
        "Resilient Distributed Datasets (RDD) are fundamental data structures of Spark. An RDD is essentially the Spark representation of a set of data, spread across multiple machines, with APIs to let you act on it.\n",
        "\n",
        "Use an RDD when:\n",
        "[(quoted from databricks)](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
        "\n",
        "- you want low-level transformation and actions and control on your dataset;\n",
        "- your data is unstructured, such as media streams or streams of text;\n",
        "- you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
        "- you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEOwsmcaryGP"
      },
      "source": [
        "RDDs have 2 operations that you can perform on them:\n",
        "- Transformations (create a new RDD)\n",
        "- Actions (return results)\n",
        "\n",
        "Note: transformations are lazy operators, they won't actually perform the transformation until an action is performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "131F8OBH-DiD"
      },
      "source": [
        "import pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLvUrYborxEK"
      },
      "source": [
        "# create a new RDD \n",
        "nums = list(range(1,1001))\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "\n",
        "rdd = sc.parallelize(nums, numSlices=10)\n",
        "rdd.getNumPartitions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkC9D1QauUWa"
      },
      "source": [
        "#### Examples of Actions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlNSWFUxw9HR"
      },
      "source": [
        "# first\n",
        "rdd.first()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzTPNXu_xPHc"
      },
      "source": [
        "# take\n",
        "rdd.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9AJBLs7xV-F"
      },
      "source": [
        "# collect\n",
        "rdd.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKfFlbTIxi7J"
      },
      "source": [
        "# grab first partition using glom\n",
        "rdd.glom().collect()[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSs7jI9tIrZX"
      },
      "source": [
        "print(type(rdd))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNQADZFRv-aJ"
      },
      "source": [
        "#### Examples of Transformations\n",
        "- map\n",
        "- filter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTDUSkbYxwgN"
      },
      "source": [
        "# map\n",
        "# use a lambda to return x+1 if x is even, else just return x\n",
        "even_rdd = rdd.map(lambda x: x + 1 if x % 2 == 0 else x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boCO3RMgxwpJ"
      },
      "source": [
        "even_rdd.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwv1yHcAyRmr"
      },
      "source": [
        "# now let's try to just return even results\n",
        "rdd.map(lambda x: x if x % 2 == 0)\n",
        "# can't really use a map for this..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZrYWGUEyoBu"
      },
      "source": [
        "# try with filter now\n",
        "only_evens = rdd.filter(lambda x: x % 2 == 0)\n",
        "only_evens.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBJchDwVzE-U"
      },
      "source": [
        "# stop your pyspark context instance\n",
        "# can't have multiple connections at once!\n",
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJh_wnqVhZ9R"
      },
      "source": [
        "# Spark DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo8LpYauxdoC"
      },
      "source": [
        "Dataframes in PySpark are the distributed collection of structured or semi-structured data. This data in Dataframe is stored in rows under named columns which is similar to the relational database tables or excel sheets. \n",
        "\n",
        "Use a Dataframe when:\n",
        "[(also quoted from databricks)](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
        "\n",
        "- you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame\n",
        "- your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame\n",
        "- you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset;\n",
        "- you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset;\n",
        "- If you are a R user, use DataFrames.\n",
        "- If you are a Python user, use DataFrames and resort back to RDDs if you need more control.\n",
        "\n",
        "Note: Machine learning algorithms are run on DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU2EAqWwHKJE"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4YRBOoJHMPf"
      },
      "source": [
        "spark = SparkSession.builder.master('local').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cCAlDu9j8wI"
      },
      "source": [
        "Need to upload data to access here!\n",
        "\n",
        "Link: https://www.kaggle.com/usdot/flight-delays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EEuVOh5HUzk"
      },
      "source": [
        "# reading in pyspark df - check file location after you upload it!\n",
        "spark_df = spark.read.csv('/flights.csv', header='true', inferSchema='true')\n",
        "\n",
        "# observing the datatype of df\n",
        "type(spark_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hos2Ek_xxvgt"
      },
      "source": [
        "# number of rows\n",
        "print(spark_df.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhMZVKjR0PRy"
      },
      "source": [
        "# number of columns\n",
        "print(len(spark_df.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj6xiwvS0VFJ"
      },
      "source": [
        "# check first five rows v1\n",
        "spark_df.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAk8Cuc60a-L"
      },
      "source": [
        "# check first five rows v2\n",
        "spark_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHi7-Tcn0cxw"
      },
      "source": [
        "# check column datatypes\n",
        "spark_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4VRqnibQeHO"
      },
      "source": [
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "\n",
        "# check for nans in each column\n",
        "spark_df.select([count(when(isnan(c), c)).alias(c) for c in spark_df.columns]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-A-eMdicBOz"
      },
      "source": [
        "# but NOT the same as nulls!\n",
        "spark_df.select([count(when(col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo1urp5e0iSA"
      },
      "source": [
        "# can groupby\n",
        "spark_df.groupby('DAY_OF_WEEK').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOnCa8LvNhQN"
      },
      "source": [
        "# only want certain columns\n",
        "spark_df = spark_df.select(col(\"MONTH\"),col(\"DAY_OF_WEEK\"), col(\"AIR_TIME\"), col('ARRIVAL_DELAY'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmn0b0HFNtWv"
      },
      "source": [
        "spark_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0X_9U7WZ1gI"
      },
      "source": [
        "spark_df.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGJl6Y6Yc_xh"
      },
      "source": [
        "# need to drop nulls in those columns\n",
        "spark_df = spark_df.na.drop(subset=[\"AIR_TIME\", \"ARRIVAL_DELAY\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaaBpEkadgcc"
      },
      "source": [
        "Now - time to prep our target! Going to predict whether there was a delay in arrival or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAtU1sKuWm7U"
      },
      "source": [
        "def prep_target(delay_value):\n",
        "  if delay_value < 0: \n",
        "    return 0\n",
        "  else: \n",
        "    return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsAW1tzdVAbo"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "# here, creating a User Defined Function, resulting in a boolean column\n",
        "udfTargetToCategory = udf(prep_target, IntegerType())\n",
        "\n",
        "preprocessed_df = spark_df.withColumn(\"delay_ind\", udfTargetToCategory(\"ARRIVAL_DELAY\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ssc6SGQZYFE"
      },
      "source": [
        "preprocessed_df.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZP32-6gHrJX"
      },
      "source": [
        "Need to encode!\n",
        "\n",
        "https://spark.apache.org/docs/2.3.0/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoderEstimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H3_8A0_0wC9"
      },
      "source": [
        "from pyspark.ml import feature\n",
        "\n",
        "ohe = feature.OneHotEncoderEstimator(inputCols=['MONTH', 'DAY_OF_WEEK'], \n",
        "                                     outputCols=['month_vec', 'day_vec'])\n",
        "ohe_hot_encoded = ohe.fit(preprocessed_df).transform(preprocessed_df)\n",
        "ohe_hot_encoded.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGxQKx8QZHFg"
      },
      "source": [
        "ohe_hot_encoded.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8smUojcU2Xi4"
      },
      "source": [
        "# only using a few of the features as inputs\n",
        "features = ['AIR_TIME', 'month_vec', 'day_vec']\n",
        "target = 'delay_ind'\n",
        "\n",
        "# need to vectorize the inputs\n",
        "vector = feature.VectorAssembler(inputCols = features, outputCol = 'features')\n",
        "vectorized_df = vector.transform(ohe_hot_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utOj9mKGNFn8"
      },
      "source": [
        "print(type(vector))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3k4g2mRNLl-"
      },
      "source": [
        "vectorized_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRj2k8PMZAFu"
      },
      "source": [
        "vectorized_df.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9LAYZE2YDfn"
      },
      "source": [
        "It appears I'm not alone in having trouble with `randomSplit`: https://medium.com/udemy-engineering/pyspark-under-the-hood-randomsplit-and-sample-inconsistencies-examined-7c6ec62644bc\n",
        "\n",
        "(I'll note, though, that my problems were not rooted in this issue!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB3qQn_s3ylX"
      },
      "source": [
        "# train test split\n",
        "# Implementing the solution discussed above\n",
        "\n",
        "persist_df = vectorized_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "train, test = persist_df.randomSplit([0.7, 0.3], seed = 11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkc1NkeXOEpm"
      },
      "source": [
        "type(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqk74er94BSr",
        "collapsed": true
      },
      "source": [
        "train.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52Z7ztcH4CgX"
      },
      "source": [
        "# Now let's try a classifier!\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'delay_ind', \n",
        "                            maxDepth = 3)\n",
        "dtModel = dt.fit(train)\n",
        "predictions = dtModel.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPNh5k5L4_ob"
      },
      "source": [
        " # Evaluate!\n",
        " from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        " evaluator = BinaryClassificationEvaluator(rawPredictionCol = 'prediction',\n",
        "                                           labelCol = 'delay_ind')\n",
        " evaluator.evaluate(predictions) # Note: ROC/AUC score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHDTmAcF_Q8v"
      },
      "source": [
        "#### Evaluate! How'd we do? Why?\n",
        "\n",
        "- .5 ROC/AUC??? Guess: is our model only predicting a single class?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUOd_EsT57pU"
      },
      "source": [
        "# Explore our predictions\n",
        "predictions.groupby('prediction').count().show()\n",
        "# note - this is the size of the test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d3kHOMi6M6x"
      },
      "source": [
        "# Explore our original data\n",
        "test.groupby('delay_ind').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHmC7F5siHX6"
      },
      "source": [
        "preprocessed_df.groupby('delay_ind').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgMAMyocAiQ4"
      },
      "source": [
        "Yup - it is only predicting the majority class.\n",
        "\n",
        "How can we fix this? Let's try undersampling our majority so our classes are more balanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwE_j9uz6V0L"
      },
      "source": [
        "# Note that these don't use square brackets!\n",
        "major_df = preprocessed_df.filter(col('delay_ind') == 0)\n",
        "minor_df = preprocessed_df.filter(col('delay_ind') == 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTCyg7tH64kg"
      },
      "source": [
        "# Down-sample - without replacement\n",
        "sampled_majority_df = major_df.sample(False, .65)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIHzsSKl7MdM"
      },
      "source": [
        "sampled_majority_df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v4e4kic7N5d"
      },
      "source": [
        "combined_df = sampled_majority_df.unionAll(minor_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWkskNKl7W7T"
      },
      "source": [
        "combined_df.groupby('delay_ind').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oeg6bcC87Z9r"
      },
      "source": [
        "combined_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQVhrJIJ7dxU"
      },
      "source": [
        "ohe = feature.OneHotEncoderEstimator(inputCols=['MONTH', 'DAY_OF_WEEK'], \n",
        "                                     outputCols=['month_vec', 'day_vec'])\n",
        "ohe_hot_encoded = ohe.fit(combined_df).transform(combined_df)\n",
        "ohe_hot_encoded.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nf9YZ6f7qeK"
      },
      "source": [
        "features = ['AIR_TIME', 'month_vec', 'day_vec']\n",
        "target = 'delay_ind'\n",
        "\n",
        "vector = feature.VectorAssembler(inputCols = features, outputCol = 'features')\n",
        "vectorized_df = vector.transform(ohe_hot_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FnD8zQd78Y0"
      },
      "source": [
        "persist_df = vectorized_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "train, test = persist_df.randomSplit([0.7, 0.3], seed = 11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_yx7HUI70CO"
      },
      "source": [
        "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'delay_ind', \n",
        "                            maxDepth = 9)\n",
        "dtModel = dt.fit(train)\n",
        "predictions = dtModel.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9nWIJOJ75Lo"
      },
      "source": [
        " evaluator = BinaryClassificationEvaluator(rawPredictionCol = 'prediction',\n",
        "                                           labelCol = 'delay_ind')\n",
        " evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuT3KOCy_yU-"
      },
      "source": [
        "#### Evaluate!\n",
        "\n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V4QmB7D8fXp"
      },
      "source": [
        "# Other Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b03r-f18jMW"
      },
      "source": [
        "## Hadoop vs Spark: which is better?\n",
        "\n",
        "> Spark has been found to run 100 times faster in-memory, and 10 times faster on disk. It’s also been used to sort 100 TB of data 3 times faster than Hadoop MapReduce on one-tenth of the machines. Spark has particularly been found to be faster on machine learning applications, such as Naive Bayes and k-means. Spark performance, as measured by processing speed, has been found to be optimal over Hadoop, for several reasons:\n",
        "- Spark is not bound by input-output concerns every time it runs a selected part of a MapReduce task. It’s proven to be much faster for applications.\n",
        "- Spark’s DAGs enable optimizations between steps. Hadoop doesn’t have any cyclical connection between MapReduce steps, meaning no performance tuning can occur at that level. However, if Spark is running on YARN with other shared services, performance might degrade and cause RAM overhead memory leaks. For this reason, if a user has a use-case of batch processing, Hadoop has been found to be the more efficient system.\n",
        "\n",
        "Using Hadoop and Spark together\n",
        "> There are several instances where you would want to use the two tools together. Despite some asking if Spark will replace Hadoop entirely because of the former’s processing power, they are meant to complement each other rather than compete"
      ]
    }
  ]
}