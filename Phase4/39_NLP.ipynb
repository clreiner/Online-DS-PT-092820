{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is natural language processing? What kinds of data falls into the \"natural language\" bucket? What types of data are similar that you've already seen? What topics have we already discussed in the curriculum that lead up to this section?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text Data\n",
    "\n",
    "Using the 'Spooky Authors' dataset: https://www.kaggle.com/c/spooky-author-identification/overview\n",
    "\n",
    "Goal: determine whether a chunk of text was written by Edgar Allen Poe, HP Lovecraft or Mary Shelley (multi-class classification problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wall of imports\n",
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's grab the dataset and look at a few aspects of this dataset (shape, some examples, etc). We'll be using just the train csv for this, for ease of use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the train set from the competition \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exploring an example of one of the texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding our target from author initials to numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing our inputs and target\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing a list of stopwords from NLTK, imported above\n",
    "# We're also using the string library add punctuation to our list\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the point of a list of stopwords? How/why will we use this list?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Bag of Words\" - Count Vectorizer\n",
    "\n",
    "Useful link to the 'User Guide' part of the documentation on this: https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intstantiating our vectorizer\n",
    "\n",
    "\n",
    "# Training on the train set, then transforming the train set\n",
    "\n",
    "# Transforming the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a classifier to use on this text - Multinomial Naive Bayes\n",
    "\n",
    "\n",
    "# Fitting the classifier\n",
    "\n",
    "\n",
    "# Getting our predictions for the train and test sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see how we did!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss! How did we do? What could we change?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're about to try this on a few different vectorizers, so let's make that easier!\n",
    "\n",
    "Here, I'm writing a function where we can provide an instantiated vectorizer, an instantiated classifer, and all of our train and test data, and the function will spit out the accuracy score and confusion matrix just like above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_vectorized_text(vectorizer, classifier, Xtrain, Xtest, ytrain, ytest):\n",
    "    '''\n",
    "    Fit and transform text data using the provided vectorizer, then fit and \n",
    "    predict with the provided classifier, in order to see the resulting\n",
    "    accuracy score and confusion matrix\n",
    "    For the Xtrain, Xtest, ytrain, ytest, expect the output of an\n",
    "    sklearn train/test split\n",
    "    -\n",
    "    Inputs:\n",
    "    vectorizer: an instantiated sklearn vectorizer\n",
    "    classifier: an instantiated sklearn classifier\n",
    "    X_train: training input data\n",
    "    X_test: testing input data\n",
    "    y_train: training true result\n",
    "    y_test: testing true result\n",
    "    -\n",
    "    Outputs: \n",
    "    train_preds: predicted results for the train set\n",
    "    test_preds: predicted results for the test set\n",
    "    '''\n",
    "\n",
    "    Xtrain_transformed = vectorizer.fit_transform(Xtrain)\n",
    "    Xtest_transformed = vectorizer.transform(Xtest)\n",
    "\n",
    "    classifier.fit(Xtrain_transformed, ytrain)\n",
    "\n",
    "    train_preds = classifier.predict(Xtrain_transformed)\n",
    "    test_preds = classifier.predict(Xtest_transformed)\n",
    "\n",
    "    print(accuracy_score(ytest, test_preds))\n",
    "    plot_confusion_matrix(classifier, Xtest_transformed, ytest,\n",
    "                          values_format=\".4g\")  # to make numbers readable\n",
    "    plt.show()\n",
    "\n",
    "    return(train_preds, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add in something that was missing from our first Count Vectorizer:\n",
    "\n",
    "Link to the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a count vectorizer that removes stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new classifier and compare the results, using our previously-defined function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the same classifier, but now with stopwords removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare/discuss: \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF: Term-Frequency - Inverse Document-Frequency\n",
    "\n",
    "Woah, that's a term and a half. What even is it?\n",
    "\n",
    "From [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html):\n",
    "\n",
    "> \"The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\"\n",
    "\n",
    "Basically, it's a statistic that hopefully reflects how important a word is in the document. By looking at the overall frequency you find how common a word is across the whole corpus, compared to the document frequency that shows how common a word is within the document in question. If a word appears often in our document, but relatively rarely in the corpus, it probably captures an important word in that specific document!\n",
    "\n",
    "In this example, the training corpus is every sentence in the `text` column in our train set, and the document is the individual sentence that we're trying to classify (per row).\n",
    "\n",
    "Reference: http://www.tfidf.com/\n",
    "\n",
    "We'll be using Sklearn's [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), which is 'equivalent to CountVectorizer followed by TfidfTransformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords_list, \n",
    "                        max_df = .95, # removes words that appear in more than 95% of docs\n",
    "                        min_df = 2, # removes words that appear 2 or fewer times\n",
    "                        use_idf=True)\n",
    "\n",
    "# Training on the train set, then transforming the train set\n",
    "\n",
    "# Transforming the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, number of rows is the length of our train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, number of rows is the length of our test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually explore what our vectorizer is grabbing from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a doc in our test set\n",
    "X_test.iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a df of tf-idf values, where each column is a word in the vocabulary\n",
    "tfidf_test_df = pd.DataFrame(tfidf_test.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing that row once it's been vectorized\n",
    "test_doc = tfidf_test_df.iloc[15]\n",
    "\n",
    "print(test_doc.loc[test_doc > 0].sort_values(ascending=False)) # Showing values > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells you that for the 16th document in our test set, the word 'wake' has the highest TF-IDF value.\n",
    "\n",
    "What does this tell you about the word \"wake\" in the this document of our test set?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare to our earlier count vectorizer for this document\n",
    "count_test_df = pd.DataFrame(count_stop_test.toarray(), columns=count_stop.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_test_doc = count_test_df.iloc[15]\n",
    "\n",
    "print(count_test_doc.loc[count_test_doc > 0].sort_values(ascending=False)) # Showing values > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to discuss here why using a TF-IDF Vectorizer might have some added benefits compared to a straight Count Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, back to classifying: \n",
    "# Using our function to compare the results...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare/discuss:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use our function to try different classifiers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare/discuss: \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- Sklearn's [Working with Text Data Tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "What else can we do with natural language data beyond text classification? \n",
    "\n",
    "- [This blog post](https://blog.aureusanalytics.com/blog/5-natural-language-processing-techniques-for-extracting-information) by Aureus Analytics provides an overview of other machine learning techniques used to extract meaning from text: Named Entity Recognition, Sentiment Analysis, Text Summarization, Aspect Mining and Topic Modeling\n",
    "\n",
    "### Neural Network Vectorizer Resources:\n",
    "\n",
    "Want to go beyond Count Vectorizers or TF-IDF to embed words for machine learning? Check out Word2Vec - a way of vectorizing text that tries to capture the relationships between words. See the image below, from [this paper](https://arxiv.org/pdf/1310.4546.pdf) from Google developers, that introduced a Skip-gram neural network model that's been utilized by Word2Vec (which is a tool you can use to implement this model). You'll note that the distance between each country and it's capital city is about the same - that distance actually has meaning, and thus you can imagine that the difference between `cat` and `kitten` would be the same as the difference between `dog` and `puppy`. Et cetera!\n",
    "\n",
    "![screenshot from a paper on the Skip-gram model from devleopers at Google, https://arxiv.org/pdf/1310.4546.pdf](images/Fig2-DsitributedRepresentationsOfWordsAndPhrasesAndTheirCompositionality.png)\n",
    "\n",
    "- [Pathmind's A.I. Wiki - A Beginner's Guide to Word2Vec](https://wiki.pathmind.com/word2vec)\n",
    "- [Chris McCormick's Word2Vec Tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
