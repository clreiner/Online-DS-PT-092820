{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Pipelines and Wrapping Up Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Pipeline?\n",
    "\n",
    "Pipelines can keep our code neat and clean all the way from gathering & cleaning our data, to creating models & fine-tuning them!\n",
    "\n",
    "**Advantages**: \n",
    "- Reduces complexity\n",
    "- Convenient \n",
    "- Flexible \n",
    "- Can help prevent mistakes (like data leakage between train and test set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easily integrate transformers and estimators, plus cross validation!\n",
    "\n",
    "<img src=\"images/grid_search_cross_validation.png\" alt=\"cross validation image from sklearn's documentation\" width=500>\n",
    "\n",
    "Why might CV be good in instances when we're doing things like searching for optimal hyperparameters...?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, plot_confusion_matrix\n",
    "\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to install category_encoders if you'd like to use it:\n",
    "# !conda install -c conda-forge category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(estimator, X_train, X_test, y_train, y_test, use_decision_function='yes'):\n",
    "    '''\n",
    "    Evaluation function to show a few scores for both the train and test set\n",
    "    Also shows a confusion matrix for the test set\n",
    "    \n",
    "    use_decision_function allows you to toggle whether you use decision_function or\n",
    "    predict_proba in order to get the output needed for roc_auc_score\n",
    "    If use_decision_function == 'skip', then it ignores calculating the roc_auc_score\n",
    "    '''\n",
    "    # grab predictions\n",
    "    train_preds = estimator.predict(X_train)\n",
    "    test_preds = estimator.predict(X_test)\n",
    "    \n",
    "    # output needed for roc_auc_score\n",
    "    if use_decision_function == 'skip': # skips calculating the roc_auc_score\n",
    "        train_out = False\n",
    "        test_out = False\n",
    "    elif use_decision_function == 'yes': # not all classifiers have decision_function\n",
    "        train_out = estimator.decision_function(X_train)\n",
    "        test_out = estimator.decision_function(X_test)\n",
    "    elif use_decision_function == 'no':\n",
    "        train_out = estimator.predict_proba(X_train)[:, 1] # proba for the 1 class\n",
    "        test_out = estimator.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        raise Exception(\"The value for use_decision_function should be 'skip', 'yes' or 'no'.\")\n",
    "\n",
    "    print(type(test_out))\n",
    "    \n",
    "    # print scores\n",
    "    print(\"Train Scores\")\n",
    "    print(\"------------\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, train_preds)}\")\n",
    "    print(f\"F1 Score: {f1_score(y_train, train_preds)}\")\n",
    "    if type(train_out) == np.ndarray:\n",
    "        print(f\"ROC-AUC: {roc_auc_score(y_train, train_out)}\")\n",
    "    print(\"----\" * 5)\n",
    "    print(\"Test Scores\")\n",
    "    print(\"-----------\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, test_preds)}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, test_preds)}\")\n",
    "    if type(test_out) == np.ndarray:\n",
    "        print(f\"ROC-AUC: {roc_auc_score(y_test, test_out)}\")\n",
    "    \n",
    "    # plot test confusion matrix\n",
    "    plot_confusion_matrix(estimator, X_test, y_test)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: https://www.kaggle.com/c/cat-in-the-dat-ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab, then explore data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring numeric cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring object cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring target distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our X and y\n",
    "\n",
    "\n",
    "# and train test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "Let's find out how hard our problem is, by throwing things at it and seeing what sticks!\n",
    "\n",
    "Biggest thing to think about - what types of columns need to be treated differently?\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up how to treat our columns\n",
    "\n",
    "# Want to grab numeric columns\n",
    "\n",
    "\n",
    "# Then grab columns with fewer than 10 unique values\n",
    "\n",
    "\n",
    "# Then grab columns with more than 10, since we won't OHE those\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, set up the preprocessing steps for each type of col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together our preprocessor using a Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just out of curiosity, let's see what this looks like \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Another Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for a new pipeline!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Time!\n",
    "\n",
    "What does it mean to decide between model types? What do we think best represents this data? What else might I want to explore or understand before deciding that?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Check out Aur√©lien Geron's notebook of an [end-to-end ml project](https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb) on his GitHub repo based around his book [_Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd ed)_](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
